workers_final<-workers%>% select(-all_of(discart))
table(workers_final$mw_worker75)[2]/nrow(workers_final)
#### Run the model once to get the lambda grid
y<- workers_final$mw_worker75
X<- model.matrix(mw_worker75 ~ .-1 -fold, workers_final )
net_logit0<- glmnet(x=X,
y=y,
family="binomial",
alpha=1,
type.measure = "class"
)
##
#plot(net_logit0)
#coef(net_logit0, s=0.0592863106   )
# the solution is probably in the middle
lambda_search<-net_logit0$lambda[20:(length(net_logit0$lambda))]
### set parallel
num_cores <-  5
cl <- makeCluster(num_cores)    # Create a cluster with available cores
registerDoParallel(cl)          # Register the parallel backend
#for (i in 1:10) {
cv_results_lasso_logit<- foreach(i= 1:10, .combine = rbind,  .packages = c( "tidyverse", "glmnet","pROC") ) %dopar% {
train<- workers_final %>%
filter(fold!=i )
test<- workers_final %>%
filter(fold==i )
y<- train$mw_worker75
X<- model.matrix(mw_worker75~.-1 -fold, train )
X_test<- model.matrix(mw_worker75~.-1 -fold, test )
y_test<-  test$mw_worker75
lasso_logit<- glmnet(x=X,
y=y,
family="binomial",
alpha=1,
lambda= lambda_search,
type.measure = "class"
)
### Test error
fold_results <- data.frame()
for (l in lambda_search){
predictions<- predict(lasso_logit, X_test,  s=l, type= "response")
fg<-predictions[y_test=="Yes"]
bg<-predictions[y_test=="No"]
pr <- pr.curve(scores.class0 = fg, scores.class1 = bg, curve = T)
areauPR<- pr$auc.integral
fold_results <- rbind(fold_results, data.frame(param = l, fold = i, auPR = areauPR))
}
fold_results
}
stopCluster(cl)
### set parallel
num_cores <-  5
cl <- makeCluster(num_cores)    # Create a cluster with available cores
registerDoParallel(cl)          # Register the parallel backend
#for (i in 1:10) {
cv_results_lasso_logit<- foreach(i= 1:10, .combine = rbind,  .packages = c( "tidyverse", "glmnet","pROC", "PRROC") ) %dopar% {
train<- workers_final %>%
filter(fold!=i )
test<- workers_final %>%
filter(fold==i )
y<- train$mw_worker75
X<- model.matrix(mw_worker75~.-1 -fold, train )
X_test<- model.matrix(mw_worker75~.-1 -fold, test )
y_test<-  test$mw_worker75
lasso_logit<- glmnet(x=X,
y=y,
family="binomial",
alpha=1,
lambda= lambda_search,
type.measure = "class"
)
### Test error
fold_results <- data.frame()
for (l in lambda_search){
predictions<- predict(lasso_logit, X_test,  s=l, type= "response")
fg<-predictions[y_test=="Yes"]
bg<-predictions[y_test=="No"]
pr <- pr.curve(scores.class0 = fg, scores.class1 = bg, curve = T)
areauPR<- pr$auc.integral
fold_results <- rbind(fold_results, data.frame(param = l, fold = i, auPR = areauPR))
}
fold_results
}
stopCluster(cl)
param_order <- unique(cv_results_lasso_logit$param)[order(unique(cv_results_lasso_logit$param))]
# Create the iteration numbers for both datasets
cv_results_lasso_logit <- cv_results_lasso_logit %>%
mutate(iteration = factor(match(param, param_order)))
summary_df <- cv_results_lasso_logit %>%
group_by(param) %>%
summarise(
mean_auc = mean(auPR),
se_auc = sd(auPR) / sqrt(n()),
iteration = unique(iteration)  # Ensure we have the same iteration numbers
)
# Combined plot with iteration number on x-axis and lambda as color scale
ggplot(cv_results_lasso_logit) +
geom_boxplot( aes(x = iteration, y = auPR, fill = param), outlier.colour = "red", outlier.shape = 16, outlier.size = 2) +
geom_point(data = summary_df, aes(x = iteration, y = mean_auc), size = 1, color = "black") +
geom_errorbar(data = summary_df, aes(x = iteration, ymin = mean_auc - se_auc, ymax = mean_auc + se_auc), width = 0.2, color = "blue") +
labs(title = "",
x = "Model",
y = "AUC",
fill = "Lambda") +
scale_fill_gradient(low = "#FFDDC1", high = "#FF5500") +  # Scale color for the fill based on lambda
theme_classic(base_size = 15) +
theme(
plot.title = element_text(hjust = 0.5),
legend.position = "right",  # Move legend to the right for better space utilization
axis.text.x = element_text(angle = 90, hjust = 1, size = 7),  # Rotate x-axis labels and adjust size
axis.text.y = element_text(size = 12),  # Adjust y-axis text size
axis.title = element_text(size = 14)
)
cv_results_lasso_logit
param_order <- unique(cv_results_lasso_logit$param)[order(unique(cv_results_lasso_logit$param))]
# Create the iteration numbers for both datasets
cv_results_lasso_logit <- cv_results_lasso_logit %>%
mutate(iteration = factor(match(param, param_order)))
summary_df <- cv_results_lasso_logit %>%
group_by(param) %>%
summarise(
mean_auc = mean(auPR),
se_auc = sd(auPR) / sqrt(n()),
iteration = unique(iteration)  # Ensure we have the same iteration numbers
)
# Combined plot with iteration number on x-axis and lambda as color scale
ggplot(cv_results_lasso_logit) +
geom_boxplot( aes(x = iteration, y = auPR, fill = param), outlier.colour = "red", outlier.shape = 16, outlier.size = 2) +
geom_point(data = summary_df, aes(x = iteration, y = mean_auc), size = 1, color = "black") +
geom_errorbar(data = summary_df, aes(x = iteration, ymin = mean_auc - se_auc, ymax = mean_auc + se_auc), width = 0.2, color = "blue") +
labs(title = "",
x = "Model",
y = "AUC",
fill = "Lambda") +
scale_fill_gradient(low = "#FFDDC1", high = "#FF5500") +  # Scale color for the fill based on lambda
theme_classic(base_size = 15) +
theme(
plot.title = element_text(hjust = 0.5),
legend.position = "right",  # Move legend to the right for better space utilization
axis.text.x = element_text(angle = 90, hjust = 1, size = 7),  # Rotate x-axis labels and adjust size
axis.text.y = element_text(size = 12),  # Adjust y-axis text size
axis.title = element_text(size = 14)
)
View(summary_df)
View(cv_results_lasso_logit)
View(summary_df)
# Combined plot with iteration number on x-axis and lambda as color scale
ggplot(cv_results_lasso_logit) +
geom_boxplot( aes(x = iteration, y = auPR, fill = param), outlier.colour = "red", outlier.shape = 16, outlier.size = 2) +
geom_point(data = summary_df, aes(x = iteration, y = mean_auc), size = 1, color = "black") +
geom_errorbar(data = summary_df, aes(x = iteration, ymin = mean_auc - se_auc, ymax = mean_auc + se_auc), width = 0.2, color = "blue") +
labs(title = "",
x = "Model",
y = "AUC",
fill = "Lambda") +
scale_fill_gradient(low = "#FFDDC1", high = "#FF5500") +  # Scale color for the fill based on lambda
theme_classic(base_size = 15) +
theme(
plot.title = element_text(hjust = 0.5),
legend.position = "right",  # Move legend to the right for better space utilization
axis.text.x = element_text(angle = 90, hjust = 1, size = 7),  # Rotate x-axis labels and adjust size
axis.text.y = element_text(size = 12),  # Adjust y-axis text size
axis.title = element_text(size = 14)
)
p_load(ggplot2)
# Combined plot with iteration number on x-axis and lambda as color scale
ggplot(cv_results_lasso_logit) +
geom_boxplot( aes(x = iteration, y = auPR, fill = param), outlier.colour = "red", outlier.shape = 16, outlier.size = 2) +
geom_point(data = summary_df, aes(x = iteration, y = mean_auc), size = 1, color = "black") +
geom_errorbar(data = summary_df, aes(x = iteration, ymin = mean_auc - se_auc, ymax = mean_auc + se_auc), width = 0.2, color = "blue") +
labs(title = "",
x = "Model",
y = "AUC",
fill = "Lambda") +
scale_fill_gradient(low = "#FFDDC1", high = "#FF5500") +  # Scale color for the fill based on lambda
theme_classic(base_size = 15) +
theme(
plot.title = element_text(hjust = 0.5),
legend.position = "right",  # Move legend to the right for better space utilization
axis.text.x = element_text(angle = 90, hjust = 1, size = 7),  # Rotate x-axis labels and adjust size
axis.text.y = element_text(size = 12),  # Adjust y-axis text size
axis.title = element_text(size = 14)
)
library(dplyr)
# Combined plot with iteration number on x-axis and lambda as color scale
ggplot(cv_results_lasso_logit) +
geom_boxplot( aes(x = iteration, y = auPR, fill = param), outlier.colour = "red", outlier.shape = 16, outlier.size = 2) +
geom_point(data = summary_df, aes(x = iteration, y = mean_auc), size = 1, color = "black") +
geom_errorbar(data = summary_df, aes(x = iteration, ymin = mean_auc - se_auc, ymax = mean_auc + se_auc), width = 0.2, color = "blue") +
labs(title = "",
x = "Model",
y = "AUC",
fill = "Lambda") +
scale_fill_gradient(low = "#FFDDC1", high = "#FF5500") +  # Scale color for the fill based on lambda
theme_classic(base_size = 15) +
theme(
plot.title = element_text(hjust = 0.5),
legend.position = "right",  # Move legend to the right for better space utilization
axis.text.x = element_text(angle = 90, hjust = 1, size = 7),  # Rotate x-axis labels and adjust size
axis.text.y = element_text(size = 12),  # Adjust y-axis text size
axis.title = element_text(size = 14)
)
str(cv_results_lasso_logit)
View(cv_results_lasso_logit)
View(summary_df)
ggplot(cv_results_lasso_logit) +
geom_boxplot( aes(x = iteration, y = auPR, fill = param), outlier.colour = "red", outlier.shape = 16, outlier.size = 2) +
geom_point(data = summary_df, aes(x = iteration, y = mean_auPR), size = 1, color = "black") +
geom_errorbar(data = summary_df, aes(x = iteration, ymin = mean_auPR - se_auPR, ymax = mean_auPR + se_auPR), width = 0.2, color = "blue")
ggplot(cv_results_lasso_logit) +
geom_boxplot( aes(x = iteration, y = auPR, fill = param), outlier.colour = "red", outlier.shape = 16, outlier.size = 2)
library(pacman)
p_load(tidyverse,
rio,
glmnet,
doParallel,
caret,
pROC,
foreach,
PRROC)
# Combined plot with iteration number on x-axis and lambda as color scale
ggplot(cv_results_lasso_logit) +
geom_boxplot( aes(x = iteration, y = auPR, fill = param), outlier.colour = "red", outlier.shape = 16, outlier.size = 2) +
geom_point(data = summary_df, aes(x = iteration, y = mean_auPR), size = 1, color = "black") +
geom_errorbar(data = summary_df, aes(x = iteration, ymin = mean_auPR - se_auPR, ymax = mean_auPR + se_auPR), width = 0.2, color = "blue") +
labs(title = "",
x = "Model",
y = "AUC",
fill = "Lambda") +
scale_fill_gradient(low = "#FFDDC1", high = "#FF5500") +  # Scale color for the fill based on lambda
theme_classic(base_size = 15) +
theme(
plot.title = element_text(hjust = 0.5),
legend.position = "right",  # Move legend to the right for better space utilization
axis.text.x = element_text(angle = 90, hjust = 1, size = 7),  # Rotate x-axis labels and adjust size
axis.text.y = element_text(size = 12),  # Adjust y-axis text size
axis.title = element_text(size = 14)
)
param_order <- unique(cv_results_lasso_logit$param)[order(unique(cv_results_lasso_logit$param))]
# Create the iteration numbers for both datasets
cv_results_lasso_logit <- cv_results_lasso_logit %>%
mutate(iteration = factor(match(param, param_order)))
summary_df <- cv_results_lasso_logit %>%
group_by(param) %>%
summarise(
mean_auPR = mean(auPR),
se_auPR = sd(auPR) / sqrt(n()),
iteration = unique(iteration)  # Ensure we have the same iteration numbers
)
# Combined plot with iteration number on x-axis and lambda as color scale
ggplot(cv_results_lasso_logit) +
geom_boxplot( aes(x = iteration, y = auPR, fill = param), outlier.colour = "red", outlier.shape = 16, outlier.size = 2) +
geom_point(data = summary_df, aes(x = iteration, y = mean_auPR), size = 1, color = "black") +
geom_errorbar(data = summary_df, aes(x = iteration, ymin = mean_auPR - se_auPR, ymax = mean_auPR + se_auPR), width = 0.2, color = "blue") +
labs(title = "",
x = "Model",
y = "AUC",
fill = "Lambda") +
scale_fill_gradient(low = "#FFDDC1", high = "#FF5500") +  # Scale color for the fill based on lambda
theme_classic(base_size = 15) +
theme(
plot.title = element_text(hjust = 0.5),
legend.position = "right",  # Move legend to the right for better space utilization
axis.text.x = element_text(angle = 90, hjust = 1, size = 7),  # Rotate x-axis labels and adjust size
axis.text.y = element_text(size = 12),  # Adjust y-axis text size
axis.title = element_text(size = 14)
)
train<- workers_final %>%
filter(fold==5 )
test<- workers_final %>%
filter(fold==1 )
y<- train$mw_worker75
X<- model.matrix(mw_worker75~.-1 -fold, train )
X_test<- model.matrix(mw_worker75~.-1 -fold, test )
y_test<-  test$mw_worker75
rf<- randomForest(x=X,
y=y,
ntree= 50,
mtry= 5,
maxnodes= 20,
importance= TRUE )
library(pacman)
p_load(tidyverse,
rio,
# rpart,
# rpart.plot,
randomForest,
# caret,
pROC,
foreach,
doParallel)
rf<- randomForest(x=X,
y=y,
ntree= 50,
mtry= 5,
maxnodes= 20,
importance= TRUE )
predictions<- predict(rf, X_test, type= "prob")
fg<-predictions[y_test=="Yes"]
bg<-predictions[y_test=="No"]
pr <- pr.curve(scores.class0 = fg, scores.class1 = bg, curve = T)
areauPR<- pr$auc.integral
areauPR
param_combinations<- expand.grid(mtry= seq(1, 22 ,3) ,
maxnodes=seq(20, 50,5))
param_combinations
p_load(tidyverse,
#  rio,
# rpart,
# rpart.plot,
randomForest,
# caret,
pROC,
foreach,
doParallel,
PRROC)
########## clean all
rm(list=ls())
########## paths
root<- "C:\\Users\\Andres Felipe\\OneDrive - Universidad de los Andes\\Research Proyects\\ML_GEIH"
data<- paste0(root, "\\data")
raw<- paste0(data, "\\raw")
geih2012<- paste0(raw, "\\GEIH\\2012")
clean<-  paste0(data, "\\clean_prediction\\2012")
############
######## Import and prepare data.
############
workers<- readRDS(paste0(clean, "\\employees_analysis.rds"))
discart<- c("house" , "household","person",   "Hogar", "P6016",
"oci", "job_type", "unionized" , "student", "monthly_mw50", "monthly_mw75",
"monthly_mw125","inglabo", "mw_worker125" , "mw_worker50" )
workers_final<-workers%>%
select(-all_of(discart))
table(workers_final$mw_worker75)[2]/nrow(workers_final)
######## random forest
### set parallel
param_combinations<- expand.grid(mtry= seq(1, 22 ,3) ,
maxnodes=seq(20, 50,5))
num_cores <-  5
cl <- makeCluster(num_cores)    # Create a cluster with available cores
registerDoParallel(cl)          # Register the parallel backend
cv_results_rf<- foreach(i= 1:10, .combine = rbind,  .packages = c( "tidyverse", "randomForest","PRROC") ) %dopar% {
train<- workers_final %>%
filter(fold!=i )
test<- workers_final %>%
filter(fold==i )
y<- train$mw_worker75
X<- model.matrix(mw_worker75~.-1 -fold, train )
X_test<- model.matrix(mw_worker75~.-1 -fold, test )
y_test<-  test$mw_worker75
### Test error
fold_results <- data.frame()
for (l in 1:nrow(param_combinations)){
rf<- randomForest(x=X,
y=y,
ntree= 100,
mtry= param_combinations$mtry[l],
maxnodes= param_combinations$maxnodes[l],
importance= TRUE )
predictions<- predict(rf, X_test, type= "prob")
fg<-predictions[y_test=="Yes"]
bg<-predictions[y_test=="No"]
pr <- pr.curve(scores.class0 = fg, scores.class1 = bg, curve = T)
areauPR<- pr$auc.integral
fold_results <- rbind(fold_results, data.frame(model = l, fold = i, auPR = areauPR))
}
fold_results
}
rm(list=ls())
########### packages
library(pacman)
p_load(tidyverse,
#  rio,
# rpart,
# rpart.plot,
randomForest,
# caret,
pROC,
foreach,
doParallel,
PRROC)
########## clean all
rm(list=ls())
########## paths
root<- "C:\\Users\\Andres Felipe\\OneDrive - Universidad de los Andes\\Research Proyects\\ML_GEIH"
data<- paste0(root, "\\data")
raw<- paste0(data, "\\raw")
geih2012<- paste0(raw, "\\GEIH\\2012")
clean<-  paste0(data, "\\clean_prediction\\2012")
workers<- readRDS(paste0(clean, "\\employees_analysis.rds"))
discart<- c("house" , "household","person",   "Hogar", "P6016",
"oci", "job_type", "unionized" , "student", "monthly_mw50", "monthly_mw75",
"monthly_mw125","inglabo", "mw_worker125" , "mw_worker50" )
workers_final<-workers%>%
select(-all_of(discart))
table(workers_final$mw_worker75)[2]/nrow(workers_final)
train<- workers_final %>%
filter(fold==5 )
test<- workers_final %>%
filter(fold==1 )
y<- train$mw_worker75
X<- model.matrix(mw_worker75~.-1 -fold, train )
X_test<- model.matrix(mw_worker75~.-1 -fold, test )
y_test<-  test$mw_worker75
rf<- randomForest(x=X,
y=y,
ntree= 50,
mtry= 5,
maxnodes= 20,
importance= TRUE )
View(rf)
rf<- randomForest(x=X,
y=y,
ntree= 50,
mtry= 5,
maxnodes= 20,
importance= FALSE )
rf<- randomForest(x=X,
y=y,
ntree= 50,
mtry= 5,
maxnodes= 20,
importance= FALSE,
nodesize= 20)
getTree(rf, k=1, labelVar=TRUE)
train<- workers_final %>%
filter(fold!=1 )
test<- workers_final %>%
filter(fold==1 )
y<- train$mw_worker75
X<- model.matrix(mw_worker75~.-1 -fold, train )
X_test<- model.matrix(mw_worker75~.-1 -fold, test )
y_test<-  test$mw_worker75
rf<- randomForest(x=X,
y=y,
ntree= 50,
mtry= 5,
maxnodes= 20,
importance= FALSE,
nodesize= 20)
getTree(rf, k=1, labelVar=TRUE)
rf<- randomForest(x=X,
y=y,
ntree= 50,
mtry= 5,
maxnodes= 20,
importance= FALSE,
nodesize= 100)
getTree(rf, k=1, labelVar=TRUE)
rf<- randomForest(x=X,
y=y,
ntree= 50,
mtry= 5,
maxnodes= 50,
importance= FALSE,
nodesize= 50)
rf
getTree(rf, k=1, labelVar=TRUE)
########## clean all
rm(list=ls())
########## paths
root<- "C:\\Users\\Andres Felipe\\OneDrive - Universidad de los Andes\\Research Proyects\\ML_GEIH"
data<- paste0(root, "\\data")
raw<- paste0(data, "\\raw")
geih2012<- paste0(raw, "\\GEIH\\2012")
clean<-  paste0(data, "\\clean_prediction\\2012")
############
######## Import and prepare data.
############
workers<- readRDS(paste0(clean, "\\employees_analysis.rds"))
discart<- c("house" , "household","person",   "Hogar", "P6016",
"oci", "job_type", "unionized" , "student", "monthly_mw50", "monthly_mw75",
"monthly_mw125","inglabo", "mw_worker125" , "mw_worker50" )
workers_final<-workers%>%
select(-all_of(discart))
table(workers_final$mw_worker75)[2]/nrow(workers_final)
######## random forest
### set parallel
param_combinations<- expand.grid(mtry= seq(1, 22 ,3) ,
maxnodes=seq(20, 50,5))
num_cores <-  5
cl <- makeCluster(num_cores)    # Create a cluster with available cores
registerDoParallel(cl)          # Register the parallel backend
cv_results_rf<- foreach(i= 1:10, .combine = rbind,  .packages = c( "tidyverse", "randomForest","PRROC") ) %dopar% {
train<- workers_final %>%
filter(fold!=i )
test<- workers_final %>%
filter(fold==i )
y<- train$mw_worker75
X<- model.matrix(mw_worker75~.-1 -fold, train )
X_test<- model.matrix(mw_worker75~.-1 -fold, test )
y_test<-  test$mw_worker75
### Test error
fold_results <- data.frame()
for (l in 1:nrow(param_combinations)){
rf<- randomForest(x=X,
y=y,
ntree= 100,
mtry= param_combinations$mtry[l],
maxnodes= param_combinations$maxnodes[l],
importance= FALSE,
nodesize= 50)
predictions<- predict(rf, X_test, type= "prob")
fg<-predictions[y_test=="Yes"]
bg<-predictions[y_test=="No"]
pr <- pr.curve(scores.class0 = fg, scores.class1 = bg, curve = T)
areauPR<- pr$auc.integral
fold_results <- rbind(fold_results, data.frame(model = l, fold = i, auPR = areauPR))
}
fold_results
}
rm(list=ls())
